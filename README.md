# âš¡ AI Pulse â€” ZYROX Intelligence Dashboard

> Real-time AI news aggregator. Latest articles from **Ben's Bites**, **The AI Rundown**, and **Reddit AI** â€” curated daily, cloud-backed via Supabase.

![Dashboard Preview](https://img.shields.io/badge/Status-Live-brightgreen) ![Python](https://img.shields.io/badge/Python-3.10%2B-blue) ![Supabase](https://img.shields.io/badge/Supabase-Connected-3ECF8E)

---

## ğŸš€ Features

- **Real-time feed** â€” Articles from the last 24 hours, auto-filtered
- **3 sources** â€” Ben's Bites, The AI Rundown, Reddit AI subreddits
- **Save articles** â€” Bookmark articles; syncs instantly to Supabase cloud
- **Unsave = delete** â€” Removing a saved article wipes it from local + cloud
- **Search** â€” Live client-side search across title, summary, author
- **Source filters** â€” Filter by individual source with one click
- **Article modal** â€” Click any card for full detail: image, tags, author, read link
- **ZYROX brand** â€” Dark brutalist design, `#BFF549` lime accent, Geist Mono typography
- **Supabase cloud** â€” All articles automatically synced after every scrape

---

## ğŸ—ï¸ Architecture

```
scraper.py  â”€â”€â†’  .tmp/raw_*.json
                       â”‚
               store.py (merge + dedup)
                       â”‚
       data/articles_store.json  â†â”€â”€ primary source of truth
                       â”‚  (background thread)
            supabase_sync.py  â”€â”€â†’  Supabase cloud
                       â”‚
           server.py (REST API on :3737)
                       â”‚
        dashboard/ (index.html + app.js + styles.css)
```

---

## âš™ï¸ Setup

### 1. Clone & install dependencies

```bash
git clone https://github.com/ShrikantHundekar/AI-made.git
cd AI-made
pip install -r requirements.txt
```

### 2. Configure environment

```bash
cp .env.example .env
```

Then edit `.env` and fill in:

| Variable | Description |
|---|---|
| `REDDIT_CLIENT_ID` | Reddit app client ID (from reddit.com/prefs/apps) |
| `REDDIT_CLIENT_SECRET` | Reddit app secret |
| `REDDIT_USER_AGENT` | Any descriptive string |
| `SUPABASE_URL` | Your Supabase project URL |
| `SUPABASE_ANON_KEY` | Your Supabase anon/public key |
| `LOOKBACK_HOURS` | How far back to show articles (default: `24`) |
| `DASHBOARD_PORT` | Local server port (default: `3737`) |

### 3. Reddit API setup

1. Go to [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)
2. Click **"Create App..."** â†’ select **`script`** type
3. Name: `AI_Pulse_Scraper`, Redirect URI: `http://localhost:8080`
4. Copy the **client ID** (under app name) + **secret** into `.env`

### 4. Run

```bash
# Windows â€” one-click startup (scrapes + starts server)
run.bat

# Or manually:
python tools/scraper.py   # fetch latest articles
python tools/store.py     # merge into local store
python server.py          # start dashboard server
```

Dashboard opens at **http://localhost:3737** ğŸ‰

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ server.py                  # HTTP server + REST API
â”œâ”€â”€ run.bat                    # One-click Windows launcher
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example               # Environment template
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ scraper.py             # Multi-source web scraper
â”‚   â”œâ”€â”€ store.py               # Local JSON store + dedup
â”‚   â””â”€â”€ supabase_sync.py       # Cloud sync (upsert / delete)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ index.html             # Dashboard HTML
â”‚   â”œâ”€â”€ styles.css             # ZYROX brand styles
â”‚   â””â”€â”€ app.js                 # Frontend logic
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ SOP-001-scraper.md     # Scraper architecture doc
â”‚   â”œâ”€â”€ SOP-002-store.md       # Store SOP
â”‚   â””â”€â”€ SOP-003-supabase.md    # Supabase integration SOP
â”‚
â””â”€â”€ data/                      # âš ï¸ gitignored â€” machine-generated
    â””â”€â”€ articles_store.json
```

---

## ğŸ›¢ï¸ Supabase Schema

Two tables: `articles` and `scrape_runs`. Full schema in `architecture/SOP-003-supabase.md`.

```sql
-- Quick test
SELECT source, COUNT(*) FROM articles GROUP BY source;
```

---

## ğŸ”„ Data Flow

| Action | Local JSON | Supabase |
|---|---|---|
| Scrape runs | Updated immediately | Upserted in background |
| Save article | `saved=true` written | `saved_at` synced |
| **Unsave article** | **Article deleted** | **Row hard-deleted** |

---

## ğŸ§° Manual Supabase Commands

```bash
# Test Supabase connection
python tools/supabase_sync.py --test

# Full sync local â†’ Supabase
python tools/supabase_sync.py

# Pull cloud â†’ local (re-seed)
python -c "from tools.supabase_sync import *; c=get_client(); pull_from_supabase(c)"
```

---

## ğŸ“‹ Roadmap

- [x] Ben's Bites scraper (HTML fallback)
- [x] The AI Rundown scraper
- [x] Reddit AI scraper (public API + PRAW)
- [x] 24h date filtering
- [x] Save / hard-delete unsave
- [x] Supabase cloud sync
- [x] ZYROX brand dashboard
- [ ] Windows Task Scheduler daily automation
- [ ] Supabase Realtime live updates

---

## ğŸ“„ License

MIT â€” built with [Antigravity](https://antigravity.dev) Ã— ZYROX.
